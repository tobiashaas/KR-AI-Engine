# ðŸ“š KRAI ENGINE - PYTHON PROCESSING HANDBOOK
**Complete Data Flow & Storage Guide for Multi-Manufacturer Document Processing**
*Version 1.0 - Production Implementation Guide*
*Created: 22. September 2025*

## ðŸŽ¯ **OVERVIEW - WAS MACHT DAS PYTHON SYSTEM?**

Das Python Processing System ist das **HerzstÃ¼ck** der KRAI Engine. Es nimmt Dokumente von **allen Herstellern** (HP, Canon, Epson, Brother, etc.) entgegen, analysiert sie intelligent und speichert die Daten strukturiert in der 16-Tabellen-Architektur fÃ¼r optimale Suchperformance.

### **ðŸ”„ High-Level Data Flow**

```
ðŸ“„ Hersteller Dokument Upload â†’ ðŸ§  Python Analysis â†’ ðŸ—ƒï¸ Structured Storage â†’ ðŸ” Intelligent Search
```

---

## ðŸ“‹ **DOCUMENT TYPES & PROCESSING STRATEGIES**

### **1. HP CPMD Database Files (.pdf)**
**Input**: HP CPMD PDF v2.1+ Dateien  
**Processing Strategy**: OCR + Error Code Extraction + Solution Mapping
**Target Tables**: `documents`, `chunks`, `error_codes`
**Uniqueness**: HP-specific structured error database in PDF format

### **2. Service Manuals (.pdf) - ALL MANUFACTURERS**
**Input**: PDF Service Manuals (HP, Canon, Epson, Brother, Xerox, etc.)
**Processing Strategy**: OCR + Section Detection + Error Code Extraction
**Target Tables**: `documents`, `chunks`, `error_codes` (extracted from content)
**Uniqueness**: Most manufacturers embed error codes IN the service manual text

### **3. Canon/Epson/Brother Error Code Processing**
**Input**: Service Manual PDFs with embedded error codes
**Processing Strategy**: 
- OCR text extraction
- Pattern recognition for error codes (E001, C-001, etc.)
- Context extraction around error codes for solutions
**Target Tables**: `documents`, `chunks`, `error_codes`
**Example Patterns**:
```text
Canon: "Error Code E001: Paper jam in rear unit"
Epson: "Error 031: Ink cartridge not recognized"  
Brother: "Error Code E-32: Fuser unit malfunction"
```

### **4. Parts Catalogs (.pdf) - ALL MANUFACTURERS**
**Input**: Parts Catalog PDFs
**Processing Strategy**: Part Number Extraction + Product Mapping
**Target Tables**: `documents`, `chunks` (with part number metadata)

#### **Multi-Manufacturer Error Code Processing:**

```python
# Universal Error Code Extraction - Works for ALL manufacturers
def extract_error_codes_from_pdf(pdf_text, manufacturer_name, document_type):
    """
    Extract error codes from PDF content (CPMD, Service Manuals, etc.)
    
    Args:
        pdf_text: OCR extracted text from PDF
        manufacturer_name: "HP", "Canon", "Epson", "Brother"
        document_type: "cpmd_database", "service_manual"
    """
    error_patterns = {
        'hp': [
            r'Error\s+Code?\s*([A-Z]?[0-9]+[-_]?[0-9]*)',  # C1234, E-5678
            r'Code\s*([A-Z][0-9]+)',                        # C123
        ],
        'canon': [
            r'Error\s+Code?\s*([E][0-9]{3,4})',            # E001, E1234
            r'Support\s+Code\s*([0-9]{4})',                # 6000
        ],
        'epson': [
            r'Error\s*([0-9]{3})',                         # 031, 079
            r'Service\s+Required\s*([0-9x]{5})',           # 0x97
        ],
        'brother': [
            r'Error\s+Code?\s*([A-Z]-?[0-9]{1,3})',        # E-32, B2
            r'([A-Z]{2}[0-9]{2})',                         # AF01
        ]
    }
    
    # CPMD specific patterns for HP
    if document_type == 'cpmd_database' and manufacturer_name.lower() == 'hp':
        # HP CPMD PDFs have structured layout with error code tables
        cpmd_patterns = [
            r'(?i)error\s+code\s*[:|\s]+([A-Z]?[0-9]+[-_]?[0-9]*)',
            r'(?i)troubleshooting\s+code\s*[:|\s]+([A-Z]?[0-9]+)',
            r'(?i)fault\s+code\s*[:|\s]+([A-Z]?[0-9]+)'
        ]
        patterns = error_patterns.get('hp', []) + cpmd_patterns
    else:
        # Standard service manual patterns
        patterns = error_patterns.get(manufacturer_name.lower(), [])
    
    # Universal fallback patterns
    if not patterns:
        patterns = [
            r'(?i)error\s+(?:code?)?\s*[:\-]?\s*([A-Z]?[0-9]+[-_]?[0-9]*)',
            r'(?i)code\s*[:\-]?\s*([A-Z]+[0-9]+)',
            r'(?i)fault\s+([0-9]+)',
        ]
    
    extracted_codes = []
    for pattern in patterns:
        matches = re.findall(pattern, pdf_text, re.IGNORECASE)
        extracted_codes.extend(matches)
    
    return list(set(extracted_codes))  # Remove duplicates

# Storage for ANY manufacturer - PDF source
for error_code in extracted_codes:
    error_record = {
        'manufacturer_id': manufacturer_id,  # HP, Canon, Epson, etc.
        'document_id': document_id,
        'error_code': error_code,
        'normalized_code': normalize_error_code(error_code, manufacturer_name),
        'error_description': extract_error_description(pdf_text, error_code),
        'solution_steps': extract_solution_steps(pdf_text, error_code),
        'source_system': document_type,  # 'cpmd_database' or 'service_manual'
        'page_references': find_page_numbers(pdf_text, error_code),
        'context_text': extract_surrounding_text(pdf_text, error_code)
    }
```

#### **Storage Flow fÃ¼r CPMD:**
```sql
-- Step 1: Create document record
INSERT INTO documents (
    file_name, document_type, manufacturer_id, 
    cpmd_version, metadata, processing_status
) VALUES (
    'HP_9025_CPMD_v2.1.xml', 'cpmd_database', hp_manufacturer_id,
    'v2.1.2024', json_metadata, 'processing'
);

-- Step 2: Extract and store error codes
INSERT INTO error_codes (
    manufacturer_id, document_id, error_code, normalized_code,
    error_description, solution_steps, affected_product_ids
) VALUES (
    hp_id, document_id, 'C1234', 'c1234',
    'Paper jam in input tray', solution_text, [model_id]
);

-- Step 3: Create semantic chunks
INSERT INTO chunks (
    document_id, chunk_index, text_chunk, 
    extracted_error_codes, normalized_error_codes, embedding
) VALUES (
    document_id, 0, chunk_text, 
    ['C1234'], ['c1234'], embedding_vector
);
```

### **2. HP Service Manuals (.pdf)**
**Input**: PDF Service Manuals
**Processing Strategy**: OCR + Section Detection + Manual Pairing
**Target Tables**: `documents`, `chunks`, `document_relationships`

#### **Data Extraction Process:**
```python
# 1. PDF Text Extraction
manual_data = {
    "title": "HP OfficeJet Pro 9025 Service Manual",
    "chapters": ["Troubleshooting", "Parts", "Maintenance"],
    "page_count": 234,
    "revision": "Rev A",
    "extracted_text": "Full manual content...",
    "error_references": ["C1234", "E-5678"]  # Cross-referenced
}

# 2. Storage Strategy:
documents â†’ Manual metadata + PDF reference
chunks â†’ Chapter-based text segments
document_relationships â†’ Automatic CPMD pairing
```

#### **Storage Flow fÃ¼r Service Manuals:**
```sql
-- Step 1: Create document record
INSERT INTO documents (
    file_name, document_type, manufacturer_id,
    total_pages, metadata, processing_status
) VALUES (
    'HP_9025_Service_Manual.pdf', 'service_manual', hp_id,
    234, json_metadata, 'processing'
);

-- Step 2: Create semantic chunks by chapter
INSERT INTO chunks (
    document_id, chunk_index, page_start, page_end,
    text_chunk, section_title, extracted_error_codes, embedding
) VALUES (
    document_id, 0, 45, 45, chapter_text,
    'Chapter 5: Troubleshooting', ['C1234'], embedding_vector
);

-- Step 3: Create intelligent CPMD + Manual pairing
INSERT INTO document_relationships (
    primary_document_id, secondary_document_id, 
    relationship_type, description
) VALUES (
    cpmd_id, manual_id, 'cpmd_manual_pair',
    'Intelligent pairing for comprehensive troubleshooting'
);
```

### **3. HP Parts Catalogs (.pdf)**
**Input**: Parts Catalog PDFs
**Processing Strategy**: Part Number Extraction + Product Mapping
**Target Tables**: `documents`, `chunks` (with part number metadata)

#### **Data Extraction Process:**
```python
# 1. Parts Catalog Processing
parts_data = {
    "catalog_title": "HP OfficeJet Pro 9025 Parts Catalog",
    "part_numbers": ["CB435A", "CE285A", "CB436A"],
    "part_descriptions": ["Toner Cartridge", "Fuser Unit"],
    "compatibility": ["HP 9025", "HP 9020"],
    "illustrations": ["Figure 1-1", "Figure 2-3"]
}

# 2. Storage Strategy:
documents â†’ Catalog metadata
chunks â†’ Part-focused text segments with part number extraction
```

---

## ðŸ—ƒï¸ **DETAILED TABLE STORAGE MAPPING**

### **Core HP Documentation Tables**

#### **ðŸ“‹ `manufacturers` Table**
**Purpose**: HP + Competitor management
**Data Source**: Configuration/Admin input
**Python Interaction**: Lookup for manufacturer_id assignment
```python
# Get HP manufacturer ID
hp_manufacturer = await get_manufacturer_by_name("HP Inc.")
manufacturer_id = hp_manufacturer['id']
```

#### **ðŸ­ `products` Table** 
**Purpose**: HP Product hierarchy (Series â†’ Model â†’ Options)
**Data Source**: Document metadata + Manual configuration
**Python Interaction**: Product detection from document content
```python
# Auto-detect product from document
detected_products = extract_product_models(document_text)
# Example: ["HP OfficeJet Pro 9025", "HP OfficeJet Pro 9020"]

# Link to existing products or create new entries
product_ids = await link_or_create_products(detected_products, manufacturer_id)
```

#### **ðŸ“„ `documents` Table**
**Purpose**: Master document registry
**Data Source**: All uploaded files
**Python Interaction**: Primary entry point for all processing
```python
# Create document record for any file type
document_record = {
    'file_name': uploaded_file.name,
    'file_hash': calculate_file_hash(file_content),
    'storage_path': storage_url,
    'document_type': detect_document_type(file_content),
    'manufacturer_id': manufacturer_id,
    'product_ids': detected_product_ids,
    'total_pages': page_count if pdf else None,
    'cpmd_version': cpmd_version if cpmd else None,
    'processing_status': 'processing',
    'metadata': extraction_metadata
}
```

#### **ðŸ“ `chunks` Table**
**Purpose**: Semantic text segments with embeddings
**Data Source**: All processed documents
**Python Interaction**: Core content storage for search
```python
# Create chunks for any document type
for i, chunk in enumerate(extracted_chunks):
    chunk_record = {
        'document_id': document_id,
        'chunk_index': i,
        'page_start': chunk.page_start,
        'page_end': chunk.page_end,
        'text_chunk': chunk.content,
        'token_count': count_tokens(chunk.content),
        'fingerprint': generate_chunk_hash(chunk.content),
        'section_title': chunk.section_title,
        'extracted_error_codes': extract_error_codes(chunk.content),
        'normalized_error_codes': normalize_error_codes(extracted_codes),
        'extracted_part_numbers': extract_part_numbers(chunk.content),
        'embedding': await generate_embedding(chunk.content),
        'ocr_confidence': chunk.ocr_confidence if ocr else None,
        'chunk_quality_score': calculate_quality_score(chunk),
        'processing_status': 'completed'
    }
```

#### **âš ï¸ `error_codes` Table - MULTI-MANUFACTURER**
**Purpose**: Error codes from ALL manufacturers (HP, Canon, Epson, Brother, etc.)
**Data Source**: CPMD XML files (HP only) + Service Manual extraction (All manufacturers)
**Python Interaction**: Universal error code extraction + manufacturer-specific normalization

```python
# Universal Error Code Storage - Works for ALL manufacturers
async def store_error_code(error_data, manufacturer_name):
    error_record = {
        'manufacturer_id': get_manufacturer_id(manufacturer_name),
        'document_id': error_data.document_id,
        'error_code': error_data.raw_code,  # "E001", "C1234", "B-32"
        'normalized_code': normalize_by_manufacturer(error_data.raw_code, manufacturer_name),
        'error_description': error_data.description,
        'solution_steps': error_data.solution_text,
        'affected_product_ids': error_data.compatible_products,
        'source_system': error_data.source,  # 'cpmd_database' or 'service_manual'
        'page_references': error_data.page_numbers,  # For manual sources
        'context_text': error_data.surrounding_text,  # Original context
        'extraction_confidence': error_data.confidence_score,
        'manufacturer_specific_data': {
            'hp': {'cpmd_version': '2.1'} if manufacturer_name == 'HP' else None,
            'canon': {'support_code_variant': True} if manufacturer_name == 'Canon' else None,
            'epson': {'service_mode_required': True} if manufacturer_name == 'Epson' else None,
            'brother': {'lcd_display_format': 'alphanumeric'} if manufacturer_name == 'Brother' else None
        }
    }
    
    await insert_error_code(error_record)

# Manufacturer-specific normalization
def normalize_by_manufacturer(error_code, manufacturer):
    normalizers = {
        'hp': lambda code: code.lower().replace('-', '').replace('_', ''),
        'canon': lambda code: code.upper().replace('E', 'e'),
        'epson': lambda code: code.zfill(3),  # Pad to 3 digits
        'brother': lambda code: code.upper().replace('-', ''),
        'default': lambda code: code.lower().strip()
    }
    
    normalizer = normalizers.get(manufacturer.lower(), normalizers['default'])
    return normalizer(error_code)
```

#### **ðŸ”— `document_relationships` Table**
**Purpose**: Intelligent document pairing (CPMD + Manual)
**Data Source**: Python intelligent pairing logic
**Python Interaction**: Automatic relationship creation
```python
# Intelligent CPMD + Manual pairing
async def create_document_relationship(cpmd_doc_id, manual_doc_id):
    # Analyze content overlap and error code references
    overlap_score = calculate_content_overlap(cpmd_content, manual_content)
    common_error_codes = find_common_error_codes(cpmd_doc, manual_doc)
    
    if overlap_score > 0.7 and len(common_error_codes) > 0:
        relationship = {
            'primary_document_id': cpmd_doc_id,
            'secondary_document_id': manual_doc_id,
            'relationship_type': 'cpmd_manual_pair',
            'description': f'Intelligent pairing: {len(common_error_codes)} common error codes',
            'priority_order': 1,
            'metadata': {
                'overlap_score': overlap_score,
                'common_error_codes': common_error_codes
            }
        }
        await store_relationship(relationship)
```

### **Advanced HP Business Logic Tables**

#### **âš™ï¸ `product_compatibility` Table**
**Purpose**: Bridge/Finisher option validation rules
**Data Source**: HP Product documentation + Configuration
**Python Interaction**: Option validation during processing
```python
# Extract option compatibility from service manuals
compatibility_rules = extract_option_rules(service_manual_content)
# Example: "Finisher X requires Bridge A"

for rule in compatibility_rules:
    compatibility_record = {
        'base_product_id': base_model_id,
        'option_product_id': option_id,
        'is_compatible': rule.is_compatible,
        'compatibility_notes': rule.description,
        'installation_notes': rule.installation_procedure,
        'option_rules': {
            'requires': rule.required_options,
            'excludes': rule.conflicting_options,
            'installation_order': rule.install_sequence
        },
        'rule_priority': rule.priority
    }
```

#### **ðŸ‘¥ `option_groups` Table**
**Purpose**: Mutual exclusion groups (Finisher/Bridge groups)
**Data Source**: HP Product specifications
**Python Interaction**: Group validation logic
```python
# Create mutual exclusion groups for HP options
finisher_group = {
    'manufacturer_id': hp_manufacturer_id,
    'group_name': 'Finisher Group',
    'group_type': 'exclusive',  # Only one finisher allowed
    'max_selections': 1,
    'min_selections': 0,
    'option_product_ids': [finisher_x_id, finisher_y_id],
    'description': 'Only one finisher can be installed per printer',
    'technical_reason': 'Physical space constraints in rear assembly'
}
```

#### **ðŸ“Š `competitive_features` & `product_features` Tables**
**Purpose**: Feature comparison framework
**Data Source**: HP Specifications + Competitive analysis
**Python Interaction**: Feature extraction from documents
```python
# Extract product features from specifications
features = extract_technical_specifications(document_content)
# Example: {"print_speed_color": 22, "max_resolution": 4800}

for feature_name, feature_value in features.items():
    feature_record = {
        'product_id': product_id,
        'feature_id': get_feature_id(feature_name),
        'feature_value': {
            'value': feature_value,
            'unit': detect_unit(feature_name),
            'verified': True
        },
        'verified': True,
        'source': 'official_manual'
    }
```

### **System Performance Tables**

#### **ðŸ“ˆ `performance_metrics` Table**
**Purpose**: Query performance tracking
**Python Interaction**: Automatic logging during processing
```python
# Log processing performance
await log_performance_metric({
    'operation_type': 'document_processing',
    'operation_name': 'cpmd_xml_parse',
    'execution_time_ms': processing_time,
    'input_size_bytes': file_size,
    'output_records': record_count,
    'success': True,
    'metadata': {'document_type': 'cpmd_database'}
})
```

#### **ðŸ” `search_logs` Table**
**Purpose**: User search analytics
**Python Interaction**: API search endpoint logging
```python
# Log search queries for analytics
await log_search_query({
    'search_query': user_query,
    'search_type': 'comprehensive_search',
    'result_count': len(search_results),
    'response_time_ms': query_time,
    'user_session_id': session_id,
    'filters_applied': search_filters
})
```

#### **ðŸ”„ `processing_jobs` Table**
**Purpose**: Document processing queue
**Python Interaction**: Job queue management
```python
# Create processing job for async handling
job = {
    'job_type': 'document_processing',
    'status': 'pending',
    'input_data': {
        'file_path': storage_path,
        'document_type': detected_type,
        'manufacturer': 'HP Inc.'
    },
    'created_by': user_id,
    'priority': calculate_job_priority(file_size, document_type)
}
```

---

## ðŸ”„ **COMPLETE PROCESSING WORKFLOW**

### **ðŸ“¥ Step 1: Document Upload & Classification**
```python
async def process_uploaded_document(file_upload, metadata):
    # 1. File validation and storage
    file_hash = calculate_file_hash(file_upload.content)
    storage_path = await store_file(file_upload, storage_provider)
    
    # 2. Document type detection
    document_type = detect_document_type(file_upload)
    # Returns: 'cpmd_database', 'service_manual', 'parts_catalog', etc.
    
    # 3. Create processing job
    job_id = await create_processing_job(storage_path, document_type, metadata)
    
    # 4. Return immediate response while processing async
    return {"job_id": job_id, "status": "processing"}
```

### **ðŸ§  Step 2: Content Analysis & Extraction - MULTI-MANUFACTURER**

```python
async def analyze_document_content(file_path, document_type, manufacturer_name):
    """Universal content analysis for all manufacturers"""
    
    if document_type == 'cpmd_database':
        # Only HP has CPMD files
        if manufacturer_name.lower() == 'hp':
            return await process_cpmd_xml(file_path)
        else:
            raise ValueError("CPMD files only available for HP")
    
    elif document_type == 'service_manual':
        # ALL manufacturers use service manuals with embedded error codes
        return await process_service_manual_universal(file_path, manufacturer_name)
    
    elif document_type == 'parts_catalog':
        # ALL manufacturers have parts catalogs
        return await process_parts_catalog_universal(file_path, manufacturer_name)
    
    else:
        return await process_generic_document(file_path, manufacturer_name)

async def process_service_manual_universal(file_path, manufacturer):
    """Extract error codes from ANY manufacturer's service manual"""
    
    # 1. Extract text (OCR if needed)
    manual_text = await extract_text_from_pdf(file_path)
    
    # 2. Manufacturer-specific error code extraction
    error_codes = extract_error_codes_from_manual(manual_text, manufacturer)
    
    # 3. Create semantic chunks with error code metadata
    chunks = create_semantic_chunks(manual_text, error_codes)
    
    # 4. Extract product references
    products = extract_product_references(manual_text, manufacturer)
    
    return {
        'type': 'service_manual',
        'manufacturer': manufacturer,
        'error_codes': error_codes,
        'chunks': chunks,
        'products': products,
        'text_content': manual_text
    }
```

### **ðŸ’¾ Step 3: Structured Data Storage**
```python
async def store_processed_data(analysis_result, document_type):
    # 1. Create master document record
    document_id = await create_document_record(analysis_result)
    
    # 2. Store document-specific data
    if document_type == 'cpmd_database':
        await store_error_codes(analysis_result.error_codes, document_id)
    
    # 3. Create semantic chunks for all document types
    await create_semantic_chunks(analysis_result.chunks, document_id)
    
    # 4. Create intelligent relationships
    await create_document_relationships(document_id, analysis_result)
    
    # 5. Update processing status
    await update_processing_status(document_id, 'completed')
```

### **ðŸ” Step 4: Search Index Updates**
```python
async def update_search_indexes(document_id):
    # 1. Generate vector embeddings for chunks
    chunks = await get_document_chunks(document_id)
    for chunk in chunks:
        embedding = await generate_embedding(chunk.text_content)
        await update_chunk_embedding(chunk.id, embedding)
    
    # 2. Update full-text search indexes (automatic via PostgreSQL)
    # 3. Update document relationship mappings
    # 4. Refresh materialized views if any
```

---

## âš™ï¸ **PYTHON CONFIGURATION MAPPING**

### **Environment Variables Usage**
```python
# HP-Specific Configuration
CPMD_SUPPORTED_VERSIONS = os.getenv('CPMD_SUPPORTED_VERSIONS', 'v2.1,v2.2,v2.3').split(',')
OPTION_VALIDATION_ENABLED = os.getenv('OPTION_VALIDATION_ENABLED', 'true').lower() == 'true'
ERROR_CODE_NORMALIZATION = os.getenv('ERROR_CODE_NORMALIZATION', 'true').lower() == 'true'

# Storage Configuration
STORAGE_PROVIDER = os.getenv('STORAGE_PROVIDER', 'supabase')
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_SERVICE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')

# AI Configuration  
EMBEDDING_PROVIDER = os.getenv('EMBEDDING_PROVIDER', 'openai')
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
SIMILARITY_THRESHOLD = float(os.getenv('SIMILARITY_THRESHOLD', '0.7'))
```

### **Database Connection Management**
```python
# Supabase Client Setup
supabase_client = create_client(
    supabase_url=SUPABASE_URL,
    supabase_key=SUPABASE_SERVICE_KEY
)

# Direct PostgreSQL for complex operations
db_connection = asyncpg.connect(
    host=DB_HOST,
    port=DB_PORT,
    user=DB_USER,
    password=DB_PASSWORD,
    database=DB_NAME
)
```

---

## ðŸŽ¯ **SUCCESS METRICS & VALIDATION**

### **Processing Success Criteria**
- **CPMD Files**: Error codes extracted and normalized successfully
- **Service Manuals**: Chapters segmented and cross-references created
- **Parts Catalogs**: Part numbers extracted with compatibility mapping
- **All Files**: Vector embeddings generated for semantic search

### **Data Quality Validation**
```python
# Validate processed data quality
async def validate_processing_quality(document_id):
    checks = {
        'chunks_created': await count_document_chunks(document_id) > 0,
        'embeddings_generated': await check_chunk_embeddings(document_id),
        'error_codes_extracted': await count_error_codes(document_id) if cpmd else True,
        'relationships_created': await check_document_relationships(document_id),
        'metadata_complete': await validate_document_metadata(document_id)
    }
    
    quality_score = sum(checks.values()) / len(checks)
    return quality_score >= 0.8  # 80% success threshold
```

---

## ðŸ“‹ **IMPLEMENTATION CHECKLIST**

### **Phase 1: Core Processing Engine**
- [ ] Document type detection algorithm
- [ ] CPMD XML parser with error code extraction
- [ ] PDF text extraction with OCR fallback
- [ ] Semantic chunking strategy implementation
- [ ] Vector embedding generation pipeline

### **Phase 2: Multi-Manufacturer Intelligence**

- [ ] Error code normalization logic for ALL manufacturers
- [ ] Manufacturer-specific error code pattern recognition
- [ ] Product model detection from content (HP, Canon, Epson, Brother)
- [ ] Cross-manufacturer option compatibility rules
- [ ] Intelligent document pairing algorithm (Service Manual + Parts Catalog)

### **Phase 3: Storage Integration**

- [ ] Supabase client integration
- [ ] Structured data insertion pipelines for all manufacturers
- [ ] Universal relationship creation logic
- [ ] Search index maintenance across all brands

### **Phase 4: Performance & Monitoring**

- [ ] Processing job queue management
- [ ] Performance metrics logging per manufacturer
- [ ] Error handling and retry logic
- [ ] Data quality validation for all document types

---

**ðŸŽ¯ Dieses Handbuch definiert exakt, wie jedes Datenelement von ALLEN Herstellern verarbeitet und in der 16-Tabellen-Architektur gespeichert wird. Ready fÃ¼r die Multi-Manufacturer Python Implementation!**